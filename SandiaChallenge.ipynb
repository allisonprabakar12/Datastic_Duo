{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5, 6, 7, 8, 9, 11, 20, 24, 25, 27, 29, 30, 32, 34, 36, 37, 38, 40]\n",
      "[10, 13, 14, 15, 16, 17, 19, 31, 35, 39]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Step 2: Load ERP Data\\npath = \"EEG_Measurements\"  # Update this path to where your ERP files are located\\ndata = []\\n\\n# Define the N400 time window (300 ms to 500 ms)\\nn400_start_time = 300  # Starting time point in milliseconds\\nn400_end_time = 500    # Ending time point in milliseconds\\n\\n# Calculate row indices for the N400 time window\\nn400_start_index = n400_start_time + 100  # +100 because time starts at -100 ms\\nn400_end_index = n400_end_time + 100\\n\\n# Iterate through the files in the specified directory\\nfor file in os.listdir(path):\\n    if \"spanish-english\" in file or \"english-english\" in file:\\n        # Load the ERP data\\n        df = pd.read_csv(os.path.join(path, file))\\n\\n        # Get the participant ID from the filename (assuming it follows the convention)\\n        participant_id = file.split(\\'_\\')[-1].split(\\'.\\')[0]  # Extract participant number\\n\\n        # Add the label based on the file name\\n        label = 1 if \"spanish-english\" in file else 0  # 1 for bilingual, 0 for monolingual\\n        \\n        # Extract the N400 window (rows from n400_start_index to n400_end_index)\\n        n400_window = df.iloc[n400_start_index:n400_end_index]\\n\\n        # Calculate features: mean, peak, and variance within the N400 window\\n        n400_mean = n400_window.mean(axis=0)\\n        n400_peak = n400_window.max(axis=0)\\n        n400_variance = n400_window.var(axis=0)\\n\\n        # Create a DataFrame for the calculated features\\n        feature_df = pd.DataFrame({\\n            \\'n400_mean\\': n400_mean,\\n            \\'n400_peak\\': n400_peak,\\n            \\'n400_variance\\': n400_variance\\n        }).T  # Transpose to have features as rows\\n\\n        # Append participant ID and label\\n        feature_df[\\'label\\'] = label\\n        feature_df[\\'participant\\'] = participant_id\\n\\n        # Append the processed data\\n        data.append(feature_df)\\n\\n# Combine all data into a single DataFrame\\nall_data = pd.concat(data, ignore_index=True)\\nprint(\"Combined Data Shape:\", all_data.shape)\\nprint(all_data.head())  # Display the first few rows of the combined data\\n\\n# Step 3: Separate Features and Labels\\nX = all_data[[\\'n400_mean\\', \\'n400_peak\\', \\'n400_variance\\']]\\ny = all_data[\\'label\\']\\n\\n# Step 4: Split Data into Train and Test Sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\\n\\n# Check shapes and data types\\nprint(\"X_train shape:\", X_train.shape)\\nprint(\"y_train shape:\", y_train.shape)\\nprint(\"X_train dtypes:\", X_train.dtypes)\\nprint(\"y_train dtypes:\", y_train.dtypes)\\n\\n# Check for NaN values\\nprint(\"NaN values in X_train:\", X_train.isnull().sum())\\nprint(\"NaN values in y_train:\", y_train.isnull().sum()) \\n\\n# Remove rows with NaN values if necessary\\nX_train = X_train.dropna()\\ny_train = y_train[X_train.index]  # Ensure y_train matches X_train after dropping NaNs\\n\\n# Step 5: Train Random Forest Classifier\\nmodel = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\\nmodel.fit(X_train, y_train)\\n\\n# Step 6: Evaluate Model\\ny_pred = model.predict(X_test)\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\\n\\n# Step 7: Feature Importances\\nfeature_importances = model.feature_importances_\\nprint(\"Feature Importances:\", feature_importances)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Step 1: Load Metadata\n",
    "meta_data = pd.read_csv(\"metadata.csv\")\n",
    "\n",
    "# Step 2: Identify participants\n",
    "# Participants who only know English\n",
    "only_english = meta_data[meta_data[['spanish', 'german', 'french']].sum(axis=1) == 0]['participant'].tolist()\n",
    "print(only_english)\n",
    "# Participants who know English and Spanish\n",
    "bilingual_spanish = meta_data[meta_data['spanish'] == 1]['participant'].tolist()\n",
    "print(bilingual_spanish)\n",
    "'''\n",
    "# Step 2: Load ERP Data\n",
    "path = \"EEG_Measurements\"  # Update this path to where your ERP files are located\n",
    "data = []\n",
    "\n",
    "# Define the N400 time window (300 ms to 500 ms)\n",
    "n400_start_time = 300  # Starting time point in milliseconds\n",
    "n400_end_time = 500    # Ending time point in milliseconds\n",
    "\n",
    "# Calculate row indices for the N400 time window\n",
    "n400_start_index = n400_start_time + 100  # +100 because time starts at -100 ms\n",
    "n400_end_index = n400_end_time + 100\n",
    "\n",
    "# Iterate through the files in the specified directory\n",
    "for file in os.listdir(path):\n",
    "    if \"spanish-english\" in file or \"english-english\" in file:\n",
    "        # Load the ERP data\n",
    "        df = pd.read_csv(os.path.join(path, file))\n",
    "\n",
    "        # Get the participant ID from the filename (assuming it follows the convention)\n",
    "        participant_id = file.split('_')[-1].split('.')[0]  # Extract participant number\n",
    "\n",
    "        # Add the label based on the file name\n",
    "        label = 1 if \"spanish-english\" in file else 0  # 1 for bilingual, 0 for monolingual\n",
    "        \n",
    "        # Extract the N400 window (rows from n400_start_index to n400_end_index)\n",
    "        n400_window = df.iloc[n400_start_index:n400_end_index]\n",
    "\n",
    "        # Calculate features: mean, peak, and variance within the N400 window\n",
    "        n400_mean = n400_window.mean(axis=0)\n",
    "        n400_peak = n400_window.max(axis=0)\n",
    "        n400_variance = n400_window.var(axis=0)\n",
    "\n",
    "        # Create a DataFrame for the calculated features\n",
    "        feature_df = pd.DataFrame({\n",
    "            'n400_mean': n400_mean,\n",
    "            'n400_peak': n400_peak,\n",
    "            'n400_variance': n400_variance\n",
    "        }).T  # Transpose to have features as rows\n",
    "\n",
    "        # Append participant ID and label\n",
    "        feature_df['label'] = label\n",
    "        feature_df['participant'] = participant_id\n",
    "\n",
    "        # Append the processed data\n",
    "        data.append(feature_df)\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "all_data = pd.concat(data, ignore_index=True)\n",
    "print(\"Combined Data Shape:\", all_data.shape)\n",
    "print(all_data.head())  # Display the first few rows of the combined data\n",
    "\n",
    "# Step 3: Separate Features and Labels\n",
    "X = all_data[['n400_mean', 'n400_peak', 'n400_variance']]\n",
    "y = all_data['label']\n",
    "\n",
    "# Step 4: Split Data into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Check shapes and data types\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_train dtypes:\", X_train.dtypes)\n",
    "print(\"y_train dtypes:\", y_train.dtypes)\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values in X_train:\", X_train.isnull().sum())\n",
    "print(\"NaN values in y_train:\", y_train.isnull().sum()) \n",
    "\n",
    "# Remove rows with NaN values if necessary\n",
    "X_train = X_train.dropna()\n",
    "y_train = y_train[X_train.index]  # Ensure y_train matches X_train after dropping NaNs\n",
    "\n",
    "# Step 5: Train Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Step 7: Feature Importances\n",
    "feature_importances = model.feature_importances_\n",
    "print(\"Feature Importances:\", feature_importances)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Spanish', 'German', 'French'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Step 2: Identify participants\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m only_english_ids \u001b[39m=\u001b[39m meta_data[meta_data[[\u001b[39m'\u001b[39;49m\u001b[39mSpanish\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mGerman\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mFrench\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mparticipant\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m bilingual_spanish_ids \u001b[39m=\u001b[39m meta_data[meta_data[\u001b[39m'\u001b[39m\u001b[39mSpanish\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mparticipant\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m \u001b[39m# Step 3: Load ERP Data\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5937\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Spanish', 'German', 'French'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Identify participants\n",
    "only_english_ids = meta_data[meta_data[['Spanish', 'German', 'French']].sum(axis=1) == 0]['participant'].tolist()\n",
    "bilingual_spanish_ids = meta_data[meta_data['Spanish'] == 1]['participant'].tolist()\n",
    "\n",
    "# Step 3: Load ERP Data\n",
    "path = \"EEG_Measurements\"  # Update this path to where your ERP files are located\n",
    "data_english_english = []  # List to store English-English data\n",
    "data_spanish_english = []  # List to store Spanish-English data\n",
    "\n",
    "# Iterate through the files in the specified directory\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        # Load the ERP data\n",
    "        df = pd.read_csv(os.path.join(path, file))\n",
    "\n",
    "        # Get the participant ID from the filename\n",
    "        participant_id = file.split('_')[-1].split('.')[0]  # Extract participant number\n",
    "        \n",
    "        if int(participant_id) in only_english_ids:\n",
    "            if \"book_english-english\" in file:\n",
    "                data_english_english.append(df)\n",
    "            elif \"book_spanish-english\" in file:\n",
    "                data_spanish_english.append(df)\n",
    "        elif int(participant_id) in bilingual_spanish_ids:\n",
    "            if \"book_english-english\" in file:\n",
    "                data_english_english.append(df)\n",
    "            elif \"book_spanish-english\" in file:\n",
    "                data_spanish_english.append(df)\n",
    "\n",
    "# Step 4: Calculate Averages for Each Condition\n",
    "# Assuming the time window is relevant for the word \"book\"\n",
    "# Adjust the electrode index range if necessary\n",
    "n400_window_start = 300  # Starting index for N400 window\n",
    "n400_window_end = 500    # Ending index for N400 window\n",
    "\n",
    "# Create lists to hold the average values for the two conditions\n",
    "averages_english = []\n",
    "averages_spanish = []\n",
    "\n",
    "# Calculate average electrode values for English-English condition\n",
    "for df in data_english_english:\n",
    "    # Calculate mean across the N400 window\n",
    "    mean_values = df.iloc[:, n400_window_start:n400_window_end].mean(axis=1)\n",
    "    averages_english.append(mean_values.mean())  # Append the average for this participant\n",
    "\n",
    "# Calculate average electrode values for Spanish-English condition\n",
    "for df in data_spanish_english:\n",
    "    # Calculate mean across the N400 window\n",
    "    mean_values = df.iloc[:, n400_window_start:n400_window_end].mean(axis=1)\n",
    "    averages_spanish.append(mean_values.mean())  # Append the average for this participant\n",
    "\n",
    "# Step 5: Create the Final DataFrame\n",
    "# Create a DataFrame with participants as columns\n",
    "participants = only_english_ids + bilingual_spanish_ids  # Combine the two participant lists\n",
    "average_df = pd.DataFrame(columns=participants, index=[\"English-English\", \"Spanish-English\"])\n",
    "\n",
    "# Fill the DataFrame with average values\n",
    "average_df.loc[\"English-English\"] = averages_english + [None] * (len(participants) - len(averages_english))\n",
    "average_df.loc[\"Spanish-English\"] = averages_spanish + [None] * (len(participants) - len(averages_spanish))\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(average_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
